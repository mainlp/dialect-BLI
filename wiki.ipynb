{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502ce2c-cd9d-406c-b82b-cfb9bb5f7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "This notebook downloads Wikipedias in different languages and extracts raw texts. Next, it creates frequence distibutions for each language and seed lists for languages other than German. Seeds are those words, which occur frequently in one of the languages, but not in German. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004cfff-d115-4ddd-8887-3ac43473fe3f",
   "metadata": {},
   "source": [
    "### 0. Install and import packages to process Wiki and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152140e7-7041-4b08-adca-0466411c872e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install wikiextractor nltk pandas spacy wikipedia wikipedia-api tqdm widgetsnbextension ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53a7a3-29f5-4127-9284-6e119a20c211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# jupyter lab hacks to support tqdm\n",
    "!jupyter nbextension install --user --py widgetsnbextension\n",
    "!jupyter nbextension enable --user --py widgetsnbextension\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccad7791-d376-45db-b68e-d45ca21f967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/katyaartemova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import caffeine \n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c2f58-3063-4ed5-a187-444107ca10ad",
   "metadata": {},
   "source": [
    "### 1. Download & extract texts from wikipedia dumps\n",
    "This takes quite some time, esp for larger dumps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba646fa-8059-4044-94fc-83d56cb119fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download wiki dumps\n",
    "# !wget https://dumps.wikimedia.org/barwiki/latest/barwiki-latest-pages-articles-multistream.xml.bz2\n",
    "# !wget https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles-multistream.xml.bz2\n",
    "# !wget https://dumps.wikimedia.org/lbwiki/latest/lbwiki-latest-pages-articles-multistream.xml.bz2\n",
    "# !wget https://dumps.wikimedia.org/alswiki/latest/alswiki-latest-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398dfcba-b0ee-47a9-8dca-2e652e53fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Preprocessing 'barwiki-latest-pages-articles-multistream.xml.bz2' to collect template definitions: this may take some time.\n",
      "INFO: Loaded 7517 templates in 3.9s\n",
      "INFO: Starting page extraction from barwiki-latest-pages-articles-multistream.xml.bz2.\n",
      "INFO: Using 7 extract processes.\n",
      "INFO: Finished 7-process extraction of 43416 articles in 10.6s (4094.4 art/s)\n",
      "INFO: Preprocessing 'dewiki-latest-pages-articles-multistream.xml.bz2' to collect template definitions: this may take some time.\n",
      "INFO: Preprocessed 100000 pages\n",
      "INFO: Preprocessed 200000 pages\n",
      "INFO: Preprocessed 300000 pages\n",
      "INFO: Preprocessed 400000 pages\n",
      "INFO: Preprocessed 500000 pages\n",
      "INFO: Preprocessed 600000 pages\n",
      "INFO: Preprocessed 700000 pages\n",
      "INFO: Preprocessed 800000 pages\n",
      "INFO: Preprocessed 900000 pages\n",
      "INFO: Preprocessed 1000000 pages\n",
      "INFO: Preprocessed 1100000 pages\n",
      "INFO: Preprocessed 1200000 pages\n",
      "INFO: Preprocessed 1300000 pages\n",
      "INFO: Preprocessed 1400000 pages\n",
      "INFO: Preprocessed 1500000 pages\n",
      "INFO: Preprocessed 1600000 pages\n",
      "INFO: Preprocessed 1700000 pages\n",
      "INFO: Preprocessed 1800000 pages\n",
      "INFO: Preprocessed 1900000 pages\n",
      "INFO: Preprocessed 2000000 pages\n",
      "INFO: Preprocessed 2100000 pages\n",
      "INFO: Preprocessed 2200000 pages\n",
      "INFO: Preprocessed 2300000 pages\n",
      "INFO: Preprocessed 2400000 pages\n",
      "INFO: Preprocessed 2500000 pages\n",
      "INFO: Preprocessed 2600000 pages\n",
      "INFO: Preprocessed 2700000 pages\n",
      "INFO: Preprocessed 2800000 pages\n",
      "INFO: Preprocessed 2900000 pages\n",
      "INFO: Preprocessed 3000000 pages\n",
      "INFO: Preprocessed 3100000 pages\n",
      "INFO: Preprocessed 3200000 pages\n",
      "INFO: Preprocessed 3300000 pages\n",
      "INFO: Preprocessed 3400000 pages\n",
      "INFO: Preprocessed 3500000 pages\n",
      "INFO: Preprocessed 3600000 pages\n",
      "INFO: Preprocessed 3700000 pages\n",
      "INFO: Preprocessed 3800000 pages\n",
      "INFO: Preprocessed 3900000 pages\n",
      "INFO: Preprocessed 4000000 pages\n",
      "INFO: Preprocessed 4100000 pages\n",
      "INFO: Preprocessed 4200000 pages\n",
      "INFO: Preprocessed 4300000 pages\n",
      "INFO: Preprocessed 4400000 pages\n",
      "INFO: Preprocessed 4500000 pages\n",
      "INFO: Preprocessed 4600000 pages\n",
      "INFO: Preprocessed 4700000 pages\n",
      "INFO: Preprocessed 4800000 pages\n",
      "INFO: Preprocessed 4900000 pages\n",
      "INFO: Preprocessed 5000000 pages\n",
      "INFO: Preprocessed 5100000 pages\n",
      "INFO: Preprocessed 5200000 pages\n",
      "INFO: Loaded 86881 templates in 580.4s\n",
      "INFO: Starting page extraction from dewiki-latest-pages-articles-multistream.xml.bz2.\n",
      "INFO: Using 7 extract processes.\n",
      "INFO: Extracted 100000 articles (1479.5 art/s)\n",
      "INFO: Extracted 200000 articles (2203.3 art/s)\n",
      "INFO: Extracted 300000 articles (2442.2 art/s)\n",
      "INFO: Extracted 400000 articles (2596.7 art/s)\n",
      "INFO: Extracted 500000 articles (2746.0 art/s)\n",
      "INFO: Extracted 600000 articles (2841.7 art/s)\n",
      "INFO: Extracted 700000 articles (2990.8 art/s)\n",
      "INFO: Extracted 800000 articles (3384.5 art/s)\n",
      "INFO: Extracted 900000 articles (3139.3 art/s)\n",
      "INFO: Extracted 1000000 articles (3187.7 art/s)\n",
      "INFO: Extracted 1100000 articles (3216.9 art/s)\n",
      "INFO: Extracted 1200000 articles (3193.5 art/s)\n",
      "INFO: Extracted 1300000 articles (3242.1 art/s)\n",
      "INFO: Extracted 1400000 articles (3284.2 art/s)\n",
      "INFO: Extracted 1500000 articles (3285.3 art/s)\n",
      "INFO: Extracted 1600000 articles (3328.7 art/s)\n",
      "INFO: Extracted 1700000 articles (3332.5 art/s)\n",
      "INFO: Extracted 1800000 articles (3346.9 art/s)\n",
      "INFO: Extracted 1900000 articles (3375.1 art/s)\n",
      "INFO: Extracted 2000000 articles (3408.7 art/s)\n",
      "INFO: Extracted 2100000 articles (3496.5 art/s)\n",
      "INFO: Extracted 2200000 articles (3507.7 art/s)\n",
      "INFO: Extracted 2300000 articles (3493.9 art/s)\n",
      "INFO: Extracted 2400000 articles (3523.3 art/s)\n",
      "INFO: Extracted 2500000 articles (3535.2 art/s)\n",
      "INFO: Extracted 2600000 articles (3458.5 art/s)\n",
      "INFO: Extracted 2700000 articles (3514.9 art/s)\n",
      "INFO: Extracted 2800000 articles (3384.7 art/s)\n",
      "INFO: Extracted 2900000 articles (3370.0 art/s)\n",
      "INFO: Extracted 3000000 articles (3401.9 art/s)\n",
      "INFO: Extracted 3100000 articles (3384.1 art/s)\n",
      "INFO: Extracted 3200000 articles (3439.5 art/s)\n",
      "INFO: Extracted 3300000 articles (3411.3 art/s)\n",
      "INFO: Extracted 3400000 articles (3458.0 art/s)\n",
      "INFO: Extracted 3500000 articles (3347.3 art/s)\n",
      "INFO: Extracted 3600000 articles (3389.6 art/s)\n",
      "INFO: Extracted 3700000 articles (3398.3 art/s)\n",
      "INFO: Extracted 3800000 articles (3378.7 art/s)\n",
      "INFO: Extracted 3900000 articles (3406.7 art/s)\n",
      "INFO: Extracted 4000000 articles (3341.1 art/s)\n",
      "INFO: Extracted 4100000 articles (3477.7 art/s)\n",
      "INFO: Extracted 4200000 articles (3435.4 art/s)\n",
      "INFO: Extracted 4300000 articles (3455.3 art/s)\n",
      "INFO: Extracted 4400000 articles (3465.5 art/s)\n",
      "INFO: Finished 7-process extraction of 4447119 articles in 1405.1s (3165.0 art/s)\n",
      "INFO: Preprocessing 'alswiki-latest-pages-articles-multistream.xml.bz2' to collect template definitions: this may take some time.\n",
      "INFO: Loaded 8658 templates in 5.7s\n",
      "INFO: Starting page extraction from alswiki-latest-pages-articles-multistream.xml.bz2.\n",
      "INFO: Using 7 extract processes.\n",
      "INFO: Finished 7-process extraction of 35090 articles in 11.1s (3154.7 art/s)\n",
      "INFO: Preprocessing 'lbwiki-latest-pages-articles-multistream.xml.bz2' to collect template definitions: this may take some time.\n",
      "INFO: Preprocessed 100000 pages\n",
      "INFO: Loaded 7663 templates in 6.0s\n",
      "INFO: Starting page extraction from lbwiki-latest-pages-articles-multistream.xml.bz2.\n",
      "INFO: Using 7 extract processes.\n",
      "INFO: Finished 7-process extraction of 75684 articles in 19.4s (3901.8 art/s)\n"
     ]
    }
   ],
   "source": [
    "# extract raw texts from wiki dumps\n",
    "!mkdir wiki\n",
    "!python3 -m wikiextractor.WikiExtractor barwiki-latest-pages-articles-multistream.xml.bz2 --json -o wiki/bar_text\n",
    "!python3 -m wikiextractor.WikiExtractor dewiki-latest-pages-articles-multistream.xml.bz2 --json -o wiki/de_text\n",
    "!python3 -m wikiextractor.WikiExtractor alswiki-latest-pages-articles-multistream.xml.bz2 --json -o wiki/als_text\n",
    "!python3 -m wikiextractor.WikiExtractor lbwiki-latest-pages-articles-multistream.xml.bz2 --json -o wiki/lb_text\n",
    "\n",
    "!rm barwiki-latest-pages-articles-multistream.xml.bz2 \n",
    "!rm dewiki-latest-pages-articles-multistream.xml.bz2\n",
    "!rm alswiki-latest-pages-articles-multistream.xml.bz2\n",
    "!rm lbwiki-latest-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7aba92-0d93-4188-9a36-74eb4ef52a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of current languagess \n",
    "langs  = ['bar','de','als','lb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2546e6-35c9-46d3-98c0-5f4998e0c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: raw: File exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4458211201044c19bd4887abd66a265a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a90bdb6a044640be5ecb6fe4f0513a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ed1dd627f94754ba65fb48221d8239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672ff62a0c514c36b4cce378d5f4a4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract raw texts to json files \n",
    "!mkdir raw \n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "\n",
    "def process_wiki(wiki_dump_file_out, wiki_dump_folder_in):\n",
    "    fnames = glob.glob(wiki_dump_folder_in)\n",
    "    with open(wiki_dump_file_out, 'w', encoding='utf-8') as out_f:\n",
    "        for filename in  tqdm(fnames, total=len(fnames)):\n",
    "            filename=filename.replace(\"\\\\\",\"/\")\n",
    "            articles = []\n",
    "\n",
    "            for line in open(filename, 'r'):\n",
    "                try:\n",
    "                    articles.append(json.loads(line))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for article in articles:\n",
    "                sentences = sent_tokenize(article['text'])\n",
    "                for sentence in sentences:\n",
    "                    out_f.write(sentence + '\\n')\n",
    "    return \n",
    "\n",
    "\n",
    "for lang in langs:\n",
    "    wiki_dump_folder_in=f'wiki/{lang}_text/**/*'\n",
    "    wiki_dump_file_out=f'raw/{lang}_wiki.txt'\n",
    "    process_wiki(wiki_dump_file_out, wiki_dump_folder_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d2db5-b1c2-4180-9c4f-71dca3c5c646",
   "metadata": {},
   "source": [
    "### 2. Frequency distribution \n",
    "Results:\n",
    "* frequency dist for each language \n",
    "* frequency dist for 'bar','de','als' languages minus words that occur in German. These couls serve as seeds further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbec667-fb34-4d80-9407-0ddf55b66d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute freq dists\n",
    "!mkdir fd\n",
    "\n",
    "def compute_freq_dist(lang):\n",
    "    wiki_dump_file_out=f'raw/{lang}_wiki.txt'\n",
    "\n",
    "    freq_dist = Counter()\n",
    "    with open(wiki_dump_file_out,encoding='utf-8') as in_file:\n",
    "        for sent in tqdm(in_file):\n",
    "            tokens = re.findall('\\w+', sent.strip().lower())\n",
    "            freq_dist.update(tokens)\n",
    "    df = pd.DataFrame.from_records(freq_dist.most_common(), columns=['token','count'])\n",
    "    df.to_csv(f'fd/{lang}_freq_dist.csv')\n",
    "\n",
    "for lang in langs:\n",
    "    compute_freq_dist(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a317b4-6508-496f-96c6-f0dcfb0c41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute seeds\n",
    "!mkdir seed\n",
    "\n",
    "def compute_seeds(lang, de_df):\n",
    "    lang_df = pd.read_csv(f'fd/{lang}_freq_dist.csv',usecols=['token','count'])\n",
    "    lang_df[~lang_df.token.isin(de_df.token)].to_csv(f'seed/{lang}_seed.csv')\n",
    "\n",
    "de_df = pd.read_csv('de_freq_dist.csv',usecols=['token','count'])\n",
    "\n",
    "for lang in langs:\n",
    "    if not lang == 'de':\n",
    "        compute_seeds(lang, de_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af733c1d-ee84-464c-ba5a-d9392ac6585f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banana",
   "language": "python",
   "name": "banana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

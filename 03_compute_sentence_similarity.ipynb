{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055ee2a-51dc-4dd3-bb6f-81bf30632ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "\n",
    "import wikipediaapi\n",
    "import json\n",
    "import jsonlines\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pathlib\n",
    "\n",
    "output_path = 'bitext/bitext'\n",
    "\n",
    "p = pathlib.Path(output_path)\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "lang = 'bar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e4358-6724-4b52-b40d-67c8a17d5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123b6eb-3c70-49df-8781-631b94a49f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_id = \"sentence-transformers/LaBSE\"\n",
    "model = SentenceTransformer(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pool = 'CLS'\n",
    "model._modules[\"1\"].pooling_mode_cls_token = True\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1d71a-aed4-43f6-9b97-d8eaad5794ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cosine_sim(sentences1, sentences2, tokenizer=tokenizer, model=model):\n",
    "    try:\n",
    "        embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "        embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embeddings1, embeddings2).to('cpu')\n",
    "        avg_sim = torch.topk(cosine_scores, k=1).values.mean().item()\n",
    "    except:\n",
    "        avg_sim = 0\n",
    "    return avg_sim\n",
    "\n",
    "def align_sentences(sentences1, sentences2, model=model, is_sentence_model=True):\n",
    "    \n",
    "    records = []\n",
    "\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2).to('cpu')\n",
    "\n",
    "    idx = torch.topk(cosine_scores, k=1).indices.flatten().tolist()\n",
    "    values = torch.topk(cosine_scores, k=1).values\n",
    "    for i, sent in enumerate(sentences1):\n",
    "        records.append([sent, sentences2[idx[i]], float(values[i])])\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367b04a-3a09-470a-8eb7-3d7afe8b80b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_feather(f'wiki/{lang}_aligned_pages')\n",
    "df.rename(columns={'de':'de_title'}, inplace=True)\n",
    "\n",
    "df.dropna(subset=f'{lang}_sentences', inplace=True)\n",
    "df.dropna(subset=f'de_sentences', inplace=True)\n",
    "\n",
    "\n",
    "records = []\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    aligned_sents = align_sentences(row[f'{lang}_sentences'].tolist(), row['de_sentences'].tolist())\n",
    "    for sentence_pair in aligned_sents:\n",
    "        records.append([row[f'{lang}_id'], row['de_id'], row[f'{lang}_title'], row['de_title']]+sentence_pair)\n",
    "parallel_sents_df = pd.DataFrame.from_records(records, columns = [f'{lang}_id', 'de_id', f'{lang}_title', 'de_title', f'{lang}_sent', 'de_sent', 'cos_sim'])\n",
    "parallel_sents_df.sort_values(by='cos_sim', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "model_id = model_id.split('/')[1]\n",
    "parallel_sents_df.reset_index(drop=True,inplace=True)\n",
    "parallel_sents_df.to_csv(f'{output_path}/{model_id}-{pool}_{lang}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c830ece-862e-46e3-8c96-810233d23b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_tokens(text):\n",
    "    import re\n",
    "    tokens = re.findall('\\w+', text)\n",
    "    return len(tokens)\n",
    "\n",
    "def replace_period(text):\n",
    "    if text.endswith('. .'):\n",
    "        text = text.replace('. .','.')\n",
    "    else:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "parallel_sents_df = pd.read_csv(f'{output_path}/{model_id}-{pool}_{lang}.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e5ce8-7038-4833-aaf6-6c5ce79e6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sents_df = pd.read_csv(f'{output_path}/{model_id}-{pool}_{lang}.csv', index_col = 0)\n",
    "\n",
    "parallel_sents_df[f'{lang}_sent'] = parallel_sents_df[f'{lang}_sent'].apply(replace_period)\n",
    "parallel_sents_df['de_sent'] = parallel_sents_df['de_sent'].apply(replace_period)\n",
    "print('replace_period', len(parallel_sents_df))\n",
    "\n",
    "parallel_sents_df[f'{lang}_num_tokens'] = parallel_sents_df[f'{lang}_sent'].apply(num_tokens)\n",
    "parallel_sents_df[f'de_num_tokens'] = parallel_sents_df[f'de_sent'].apply(num_tokens)\n",
    "print('num tokens',len(parallel_sents_df))\n",
    "\n",
    "\n",
    "parallel_sents_df = parallel_sents_df[parallel_sents_df['de_num_tokens'] >= 5]\n",
    "parallel_sents_df = parallel_sents_df[parallel_sents_df['de_num_tokens'] <= 20]\n",
    "print('filter de num tokens', len(parallel_sents_df))\n",
    "\n",
    "parallel_sents_df = parallel_sents_df[parallel_sents_df[f'{lang}_num_tokens'] >= 5]\n",
    "parallel_sents_df = parallel_sents_df[parallel_sents_df[f'{lang}_num_tokens'] <= 20]\n",
    "print(f'filter {lang} num tokens', len(parallel_sents_df))\n",
    "\n",
    "parallel_sents_df = parallel_sents_df[~parallel_sents_df['de_sent'].str.contains('\\[\\]')]\n",
    "parallel_sents_df = parallel_sents_df[~parallel_sents_df[f'{lang}_sent'].str.contains('\\[\\]')]\n",
    "print(f'filter braces', len(parallel_sents_df))\n",
    "\n",
    "parallel_sents_df = parallel_sents_df[~parallel_sents_df['de_sent'].str.contains('\\(\\)')]\n",
    "parallel_sents_df = parallel_sents_df[~parallel_sents_df[f'{lang}_sent'].str.contains('\\(\\)')]\n",
    "print(f'filter braces', len(parallel_sents_df))\n",
    "\n",
    "parallel_sents_df = parallel_sents_df[~parallel_sents_df['de_sent'].str.endswith(':.')]\n",
    "parallel_sents_df = parallel_sents_df[~parallel_sents_df[f'{lang}_sent'].str.endswith(':.')]\n",
    "print(f'filter incomplete sents', len(parallel_sents_df))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parallel_sents_df.to_csv(f'{output_path}/{model_id}-{pool}_{lang}_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-banana] *",
   "language": "python",
   "name": "conda-env-.conda-banana-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
